{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import torch\n",
    "from IPython import display\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.utils.data as Data\n",
    "# pytorch neural network\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# 首先生成简单人工数据集\n",
    "num_inputs = 2\n",
    "num_examples = 1000\n",
    "true_w = [2, -3.4]\n",
    "true_b = 4.2\n",
    "features = torch.tensor(np.random.normal(0, 1, (num_examples, num_inputs)), dtype=torch.float)\n",
    "labels = true_w[0] * features[:, 0] + true_w[1] * features[:, 1] + true_b\n",
    "labels += torch.tensor(np.random.normal(0, 0.01, size=labels.size()), dtype=torch.float)\n",
    "\n",
    "# 读取数据集，每次随机读取 batch_size 大小的数据集\n",
    "batch_size = 10\n",
    "# 将训练数据集的 特征 & 标签 组合\n",
    "dataset = Data.TensorDataset(features, labels)\n",
    "# 随机读取 小批量数据\n",
    "# data_iter 和 此前我们手写的 data_iter函数效果一样\n",
    "data_iter = Data.DataLoader(dataset, batch_size, shuffle=True)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 定义模型\n",
    "pytorch提供大量预定义的层，我们只需关注使用哪些层来构造模型，下面将介绍如何使用PyTorch更简洁定义 线性回归。<br><br>\n",
    "首先，导入`torch.nn`模块。实际上，“nn”是neural networks（神经网络）的缩写。顾名思义，该模块定义了大量神经网络的层。之前我们已经用过了`autograd`，而`nn`就是利用`autograd`来定义模型。`nn`的核心数据结构是`Module`，它是一个抽象概念，既可以表示神经网络中的某个**层**（layer），也可以表示一个包含很**多层的神经网络**。在实际使用中，最常见的做法是继承`nn.Module`，撰写自己的网络层。一个`nn.Module`实例应该包含一些层以及返回输出的前向传播（forward）方法。下面先来看看如何用`nn.Module`实现一个线性回归模型。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class LinearNet(nn.Module):\n",
    "    def __init__(self, n_features):\n",
    "        super(LinearNet, self).__init__()\n",
    "        # 线性层，输入n_features 个特征，输出1个特征\n",
    "        # 这里的 self.linear == 之前我们手写的 linreg\n",
    "        self.linear = nn.Linear(n_features, 1)\n",
    "        # 后面添加 weight 和 bias\n",
    "        self.weight = self.linear.weight\n",
    "        self.bias = self.linear.bias\n",
    "        # forward 定义前向传播\n",
    "    def forward(self, x):\n",
    "        y = self.linear(x)\n",
    "        return y\n",
    "\n",
    "net = LinearNet(num_inputs)\n",
    "print(net)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 完全自定义模型\n",
    "class LinearNetMy(nn.Module):\n",
    "    def __init__(self, n_input, n_output):\n",
    "        super(LinearNetMy, self).__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(n_input, n_output))\n",
    "        # self.bias = nn.Parameter(torch.randn(n_output))\n",
    "        self.bias = nn.Parameter(torch.randn(1, n_output))\n",
    "    def forward(self, x):\n",
    "        y = torch.matmul(x, self.weight) + self.bias\n",
    "        return y"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "事实上，我们还可以用 `nn.Sequential` 更加方便搭建网络，`Sequential`是一个有序容器，网络层将按照 传入 `Sequential` 的顺序以此被添加到计算图中"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 写法1\n",
    "net = nn.Sequential(\n",
    "    # 线性层，输入 num_inputs 个特征，输出 1 个特征\n",
    "    nn.Linear(num_inputs, 1)\n",
    "    # 此处还可以添加更多 网络层\n",
    ")\n",
    "\n",
    "# 写法2\n",
    "net = nn.Sequential()\n",
    "net.add_module('linear', nn.Linear(num_inputs, 1))\n",
    "# net.add_module ...\n",
    "\n",
    "# 写法3\n",
    "from collections import OrderedDict\n",
    "net = nn.Sequential(OrderedDict([\n",
    "    ('linear', nn.Linear(num_inputs, 1))\n",
    "    # ......\n",
    "]))\n",
    "\n",
    "print(net)\n",
    "print(net[0])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "可以通过 `net.parameters()` 查看模型所有的可学习 参数，此函数将返回一个生成器\n",
    "线性层 其实就是一个单层神经网络，而且输入特征 与 输出特征 全连接，线性层又叫 全连接层"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for param in net.parameters():\n",
    "    print(param)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 初始化模型参数\n",
    "在使用`net`前，我们需要初始化模型参数，如线性回归模型中的权重和偏差。PyTorch在`init`模块中提供了多种参数初始化方法。这里的init是initializer的缩写形式。我们通过`init.normal_`将权重参数每个元素初始化为随机采样于均值为0、标准差为0.01的正态分布。偏差会初始化为零。<br>\n",
    "> 注：如果这里的`net`是通过继承nn.Module类自定义的，那么上面代码会报错，`net[0].weight`应改为`net.linear.weight`，`bias`亦然。因为`net[0]`这样根据下标访问子模块的写法只有当`net`是个`ModuleList`或者`Sequential`实例时才可以。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from torch.nn import init\n",
    "\n",
    "init.normal_(net[0].weight, mean=0, std=0.01)\n",
    "init.constant_(net[0].bias, val=0) # 也可以直接修改 bias 的 data: net[0].bias.data.fill_(0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 定义损失函数\n",
    "PyTorch在`nn`模块中提供了各种损失函数，这些损失函数可看作是一种特殊的层，PyTorch也将这些损失函数实现为nn.Module的子类。我们现在使用它提供的均方误差损失作为模型的损失函数。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "loss = nn.MSELoss() # 该 loss 已经求和"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 定义优化算法\n",
    "同样，我们也无须自己实现小批量随机梯度下降算法。`torch.optim`模块提供了很多常用的优化算法比如`SGD`、`Adam`和`RMSProp`等。下面我们创建一个用于优化`net`所有参数的优化器实例，并指定学习率为0.03的小批量随机梯度下降（SGD）为优化算法。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.03)\n",
    "print(optimizer)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "我们还可以为不同 网络层 设置不同的学习率，这在finetune时经常用到\n",
    "```python\n",
    "optimizer = optim.SGD([\n",
    "    # 如果对某个参数不指定学习率，就使用最外层的默认学习率\n",
    "    {'params': net.subnet1.parameters()}, # lr = 0.03\n",
    "    {'params': net.subnet2.parameters(), 'lr': 0.01}\n",
    "], lr=0.03)\n",
    "```\n",
    "有时候我们不想让学习率固定成一个常数，那如何调整学习率呢？主要有两种做法。一种是修改optimizer.param_groups中对应的学习率，另一种是更简单也是较为推荐的做法——新建优化器，由于optimizer十分轻量级，构建开销很小，故而可以构建新的optimizer。但是后者对于使用动量的优化器（如Adam），会丢失动量等状态信息，可能会造成损失函数的收敛出现震荡等情况。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# 调整学习率\n",
    "for param_group in optimizer.param_groups:\n",
    "    param_group['lr'] *= 0.1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 模型训练"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "num_epochs = 3\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    for X, y in data_iter:\n",
    "        output = net(X)\n",
    "        l = loss(output, y.view(-1, 1))\n",
    "        optimizer.zero_grad() # 梯度清零，等价于 net.zero_grad()\n",
    "        l.backward() # 向参数 累计梯度\n",
    "        optimizer.step() # 根据当前梯度 优化\n",
    "    print('epoch %d, loss: %f' % (epoch, l.item()))\n",
    "\n",
    "# 下面对最终迭代出来的 参数进行评估\n",
    "dense = net[0]\n",
    "print(true_w, dense.weight)\n",
    "print(true_b, dense.bias)\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "90fb90bed8bda3fdee481d4ac349c2c91be0ad782309a9770e9d87efd85f40c2"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
