{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import sys\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torch.utils.data as Data\n",
    "# 数据集继续使用 Fasion-MNIST\n",
    "# 使用多层感知机对图像进行分类\n",
    "batch_size = 256\n",
    "mnist_train = torchvision.datasets.FashionMNIST(root='/Users/milktime/AnacondaProjects/deeplearning/Datasets/FashionMNIST', \\\n",
    "    train=True, download=False, transform=transforms.ToTensor())\n",
    "mnist_test = torchvision.datasets.FashionMNIST(root='/Users/milktime/AnacondaProjects/deeplearning/Datasets/FashionMNIST', \\\n",
    "    train=False, download=False, transform=transforms.ToTensor())\n",
    "num_workers = 4\n",
    "train_iter = Data.DataLoader(mnist_train, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "test_iter = Data.DataLoader(mnist_test, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP模型示意图\n",
    "![avatar](https://tangshusen.me/Dive-into-DL-PyTorch/img/chapter03/3.8_mlp.svg) <br>\n",
    "有多层，但由于每一层都是全连接（线性），因此，最终的多层网络结构整体上仍然是线性的\n",
    "所以要在每一层后面加上激活函数，常见的激活函数有 `ReLU` `Sigmoid` `tanh` 可求偏导，作用在输出结果，然后作为下一层的输入\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义模型参数\n",
    "# 设置 超参数 隐藏层单元数为 256\n",
    "num_inputs, num_outputs, num_hiddens = 784, 10, 256\n",
    "\n",
    "W1 = torch.tensor(np.random.normal(0, 0.01, (num_inputs, num_hiddens)), dtype=torch.float)\n",
    "b1 = torch.zeros(num_hiddens, dtype=torch.float)\n",
    "W2 = torch.tensor(np.random.normal(0, 0.01, (num_hiddens, num_outputs)), dtype=torch.float)\n",
    "b2 = torch.zeros(num_outputs, dtype=torch.float)\n",
    "\n",
    "params = [W1, b1, W2, b2]\n",
    "for param in params:\n",
    "    param.requires_grad_(requires_grad=True)\n",
    "\n",
    "# 定义激活函数\n",
    "# out_i = max(tensor_i, other_i)\n",
    "def relu(X):\n",
    "    # 利用广播机制，X的每一个元素和0比较，取最大值\n",
    "    return torch.max(input=X, other=torch.tensor(0.0))\n",
    "\n",
    "# 定义模型\n",
    "def net(X):\n",
    "    # the shape of X is (batch_size, 1, 28, 28)\n",
    "    X = X.view(-1, num_inputs)\n",
    "    H = relu(torch.mm(X, W1) + b1)\n",
    "    return torch.mm(H, W2) + b2\n",
    "\n",
    "# 定义损失函数\n",
    "# 因为是多分类器，所以使用cross entropy\n",
    "# loss 是 每个训练样本的平均 loss，已经除过 batch_size了\n",
    "# pytorch库的 crossentropy集成了 softmax + CrossEntropy\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# 实现梯度下降优化函数\n",
    "def sgd(params, lr):\n",
    "    for param in params:\n",
    "        # 注意这里用的是 param.data，避免干扰自动计算的梯度\n",
    "        param.data -= lr * param.grad\n",
    "\n",
    "# 评估 网络模型 net 上，测试集的准确率\n",
    "def evaluate_accuracy(data_iter, net):\n",
    "    acc_sum, n = 0.0, 0\n",
    "    for X, y in data_iter:\n",
    "        y_hat = net(X)\n",
    "        acc_sum += (y_hat.argmax(dim=1) == y).sum()\n",
    "        n += X.shape[0]\n",
    "    return acc_sum / n\n",
    "\n",
    "\n",
    "# 训练模型\n",
    "num_epochs, lr = 5, 0.5\n",
    "for epoch in range(num_epochs):\n",
    "    train_l_sum, train_acc_sum, n = 0.0, 0.0, 0\n",
    "    for X, y in train_iter:\n",
    "        # 每个batch 的样本标签 组成1维向量\n",
    "        y_hat = net(X)\n",
    "        l = loss(y_hat, y)\n",
    "        l.backward() # 计算梯度\n",
    "        sgd(params, lr) # 梯度下降优化\n",
    "        for param in params:\n",
    "            param.grad.data.zero_() # 清空梯度\n",
    "        train_l_sum += l.item()\n",
    "        train_acc_sum += (y_hat.argmax(dim=1) == y).sum().item()\n",
    "        n += X.shape[0]\n",
    "    test_acc = evaluate_accuracy(test_iter, net)\n",
    "    print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f'\n",
    "              % (epoch + 1, train_l_sum / n, train_acc_sum / n, test_acc))\n",
    "\n",
    "\n",
    "# 画图\n",
    "from IPython import display\n",
    "from matplotlib import pyplot as plt\n",
    "# 下面两个函数画图相关\n",
    "def use_svg_display():\n",
    "    display.set_matplotlib_formats('svg')\n",
    "\n",
    "def set_figsize(figsize=(3.5, 2.5)):\n",
    "    use_svg_display()\n",
    "    plt.rcParams['figure.figsize'] = figsize\n",
    "\n",
    "# 将数值标签 转换为 相应的文本标签\n",
    "def get_fashion_mnist_labels(labels):\n",
    "    text_labels = ['t-shirt', 'trouser', 'pullover', 'dress', 'coat',\n",
    "                   'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']\n",
    "    return [text_labels[int(i)] for i in labels]\n",
    "\n",
    "def show_fashion_mnist(images, labels):\n",
    "    use_svg_display()\n",
    "    # 这里的_ 表示我们忽略（不使用）的变量\n",
    "    _, figs = plt.subplots(1, len(images), figsize=(12, 12))\n",
    "    for f, img, lbl in zip(figs, images, labels):\n",
    "        f.imshow(img.view((28, 28)).numpy())\n",
    "        f.set_title(lbl)\n",
    "        f.axes.get_xaxis().set_visible(False)\n",
    "        f.axes.get_yaxis().set_visible(False)\n",
    "    plt.show()\n",
    "\n",
    "X, y = iter(test_iter).next()\n",
    "true_labels = get_fashion_mnist_labels(y)\n",
    "pred_labels = get_fashion_mnist_labels(net(X).argmax(dim=1))\n",
    "titles = [true + '\\n' + pred for true, pred in zip(true_labels, pred_labels)]\n",
    "show_fashion_mnist(X[0:9], titles[0:9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP 借助 pytorch 简洁实现\n",
    "from torch import nn\n",
    "from torch import init\n",
    "\n",
    "# 定义 数据处理层，将二为图片展开\n",
    "class FlattenLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FlattenLayer, self).__init__()\n",
    "    def forward(self, x):\n",
    "        return x.view(x.shape[0], -1)\n",
    "\n",
    "from collections import OrderedDict\n",
    "net = nn.Sequential(\n",
    "    OrderedDict([\n",
    "        ('flatten', FlattenLayer()), \n",
    "        ('hidden', nn.Linear(num_inputs, num_hiddens)), \n",
    "        ('activation', nn.ReLU()), \n",
    "        ('linear', nn.Linear(num_hiddens, num_outputs))\n",
    "    ])\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "90fb90bed8bda3fdee481d4ac349c2c91be0ad782309a9770e9d87efd85f40c2"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
